# # Integration tests for ingestion flow
# import pytest
# from fastapi.testclient import TestClient
# import os
# import io
# import math

# # Import the FastAPI app from the ingestion service
# try:
#     from services.ingestion.app.app import app, storage
# except ImportError:
#     app = None
#     storage = None

# # --- Dependency Stubs/Mocks/Fakes for Fast, Isolated Tests ---

# # LLM (OpenRouter) Stub
# class StubLLMClient:
#     def generate(self, prompt, **kwargs):
#         return "stubbed LLM response"

# # Vector DB (Qdrant) Fake
# class FakeQdrantClient:
#     def __init__(self):
#         self.vectors = {}
#     def upsert(self, doc_id, vector):
#         self.vectors[doc_id] = vector
#         return True
#     def search(self, query):
#         return ["fake-doc-id"]

# # RabbitMQ Fake/Mock (in-memory queue)
# class FakeRabbitMQ:
#     def __init__(self):
#         self.queue = []
#     def publish_event(self, event_type, event, correlation_id=None):
#         self.queue.append((event_type, event, correlation_id))
#         return True
#     def get_events(self):
#         return self.queue

# # Retrieval Service Mock
# class MockRetrievalService:
#     def retrieve(self, query):
#         return {"result": "mocked retrieval", "query": query}

# # External API Stub
# class StubExternalAPI:
#     def get(self, url, **kwargs):
#         return {"status": "stubbed", "url": url}


# # Sample document fixture
# @pytest.fixture
# def sample_document():
#     return {
#         "id": "doc-001",
#         "title": "General Regulations",
#         "file_path": "/tmp/general.pdf",
#         "content": "Exam policy states ...",
#         "page_count": 20,
#         "source_url": "https://lancaster.ac.uk/.../general.pdf"
#     }

# # Dummy extractor
# class DummyExtractor:
#     def extract(self, file_path):
#         return {
#             "text": "Exam policy states ...",
#             "metadata": {
#                 "title": "General Regulations",
#                 "page_count": 20,
#                 "source_url": "https://lancaster.ac.uk/.../general.pdf"
#             }
#         }

# # Dummy chunker
# class DummyChunker:
#     def chunk(self, text):
#         return [
#             {"text": "Exam policy states ...", "chunk_index": 0},
#             {"text": "...", "chunk_index": 1}
#         ]

# def cosine_sim(a, b):
#     dot = sum(x*y for x, y in zip(a, b))
#     norm_a = math.sqrt(sum(x*x for x in a))
#     norm_b = math.sqrt(sum(y*y for y in b))
#     return dot / (norm_a * norm_b) if norm_a and norm_b else 0.0

# class FakeVectorDatabase:
#     """Fake: In-memory vector DB (no Qdrant needed)"""

#     def __init__(self):
#         self.chunks = []

# 	def insert_chunks(self, chunks, embeddings, metadata):
# 		self.chunks = []
# 		for i, chunk in enumerate(chunks):
# 			entry = {
# 				"text": chunk["text"] if isinstance(chunk, dict) and "text" in chunk else chunk,
# 				"embedding": embeddings[i],
# 				"metadata": metadata[i] if isinstance(metadata, list) else metadata
# 			}
# 			if isinstance(chunk, dict) and "chunk_index" in chunk:
# 				entry["chunk_index"] = chunk["chunk_index"]
# 			self.chunks.append(entry)
# 		return True

#     def search(self, query_embedding, top_k=5):
#         scores = [cosine_sim(query_embedding, c["embedding"]) for c in self.chunks]
#         sorted_results = sorted(
#             zip(self.chunks, scores),
#             key=lambda x: x[1], reverse=True
#         )
#         return [chunk for chunk, score in sorted_results[:top_k]]

# # Replace DummyDB with FakeVectorDatabase in tests
# def test_ingestion_flow(sample_document):
# 	extractor = DummyExtractor()
# 	chunker = DummyChunker()
# 	db = FakeVectorDatabase()

# 	# Step 1: Extract document
# 	extraction_result = extractor.extract(sample_document["file_path"])
# 	assert extraction_result["text"] == sample_document["content"]
# 	assert extraction_result["metadata"]["title"] == sample_document["title"]

# 	# Step 2: Chunk text
# 	chunks = chunker.chunk(extraction_result["text"])
# 	assert len(chunks) > 0
# 	assert chunks[0]["chunk_index"] == 0

# 	# Step 3: Store chunks in DB
# 	embeddings = [[1, 0]] * len(chunks)  # Dummy embeddings
# 	success = db.insert_chunks(chunks, embeddings, extraction_result["metadata"])
# 	assert success
# 	stored_chunks = db.chunks
# 	assert stored_chunks == [{"text": chunk["text"], "embedding": [1, 0], "metadata": extraction_result["metadata"]} for chunk in chunks]

# # Edge case: Empty document
# def test_ingestion_empty_document():
# 	class EmptyExtractor:
# 		def extract(self, file_path):
# 			return {"text": "", "metadata": {"title": "", "page_count": 0, "source_url": ""}}
# 	class DummyChunker:
# 		def chunk(self, text):
# 			return []
# 	extractor = EmptyExtractor()
# 	chunker = DummyChunker()
# 	db = FakeVectorDatabase()
# 	extraction_result = extractor.extract("/tmp/empty.pdf")
# 	assert extraction_result["text"] == ""
# 	chunks = chunker.chunk(extraction_result["text"])
# 	assert chunks == []

# # Edge case: Corrupted/unreadable file
# def test_ingestion_corrupted_file():
# 	class CorruptExtractor:
# 		def extract(self, file_path):
# 			raise IOError("File cannot be read")
# 	extractor = CorruptExtractor()
# 	try:
# 		extractor.extract("/tmp/corrupt.pdf")
# 		assert False, "Should have raised IOError"
# 	except IOError:
# 		pass

# # Edge case: Whitespace-only document
# def test_ingestion_whitespace_document():
# 	class WhitespaceExtractor:
# 		def extract(self, file_path):
# 			return {"text": "   \n\n   ", "metadata": {"title": "Whitespace", "page_count": 1, "source_url": ""}}
# 	class DummyChunker:
# 		def chunk(self, text):
# 			return []
# 	extractor = WhitespaceExtractor()
# 	chunker = DummyChunker()
# 	db = FakeVectorDatabase()
# 	extraction_result = extractor.extract("/tmp/white.pdf")
# 	assert extraction_result["text"].strip() == ""
# 	chunks = chunker.chunk(extraction_result["text"])
# 	assert chunks == []

# # Edge case: Very large document
# def test_ingestion_large_document():
# 	class LargeExtractor:
# 		def extract(self, file_path):
# 			return {"text": "A" * 10000, "metadata": {"title": "Big", "page_count": 100, "source_url": ""}}
# 	class DummyChunker:
# 		def chunk(self, text):
# 			return [{"text": text[i:i+1000], "chunk_index": i//1000} for i in range(0, len(text), 1000)]
# 	extractor = LargeExtractor()
# 	chunker = DummyChunker()
# 	db = FakeVectorDatabase()
# 	extraction_result = extractor.extract("/tmp/big.pdf")
# 	chunks = chunker.chunk(extraction_result["text"])
# 	embeddings = [[1, 0]] * len(chunks)  # Dummy embeddings
# 	db.insert_chunks(chunks, embeddings, extraction_result["metadata"])
# 	assert len(chunks) == 10

# # Edge case: Unicode document
# def test_ingestion_unicode_document():
# 	class UnicodeExtractor:
# 		def extract(self, file_path):
# 			return {"text": "ðŸ˜€ðŸ˜€ðŸ˜€", "metadata": {"title": "Emoji", "page_count": 1, "source_url": ""}}
# 	class DummyChunker:
# 		def chunk(self, text):
# 			return [{"text": text, "chunk_index": 0}]
# 	extractor = UnicodeExtractor()
# 	chunker = DummyChunker()
# 	db = FakeVectorDatabase()
# 	extraction_result = extractor.extract("/tmp/emoji.pdf")
# 	chunks = chunker.chunk(extraction_result["text"])
# 	embeddings = [[1, 0]] * len(chunks)  # Dummy embeddings
# 	db.insert_chunks(chunks, embeddings, extraction_result["metadata"])
# 	assert "ðŸ˜€" in chunks[0]["text"]

# # Edge case: Missing metadata fields
# def test_ingestion_missing_metadata():
# 	class MissingMetaExtractor:
# 		def extract(self, file_path):
# 			return {"text": "Text", "metadata": {}}
# 	extractor = MissingMetaExtractor()
# 	extraction_result = extractor.extract("/tmp/missingmeta.pdf")
# 	assert isinstance(extraction_result["metadata"], dict)

# # Edge case: Duplicate document ingestion
# def test_ingestion_duplicate_document():
# 	class DummyExtractor:
# 		def extract(self, file_path):
# 			return {"text": "Text", "metadata": {"title": "Dup", "page_count": 1, "source_url": ""}}
# 	class DummyChunker:
# 		def chunk(self, text):
# 			return [{"text": text, "chunk_index": 0}]
# 	db = FakeVectorDatabase()
# 	extractor = DummyExtractor()
# 	chunker = DummyChunker()
# 	extraction_result1 = extractor.extract("/tmp/dup.pdf")
# 	extraction_result2 = extractor.extract("/tmp/dup.pdf")
# 	chunks1 = chunker.chunk(extraction_result1["text"])
# 	chunks2 = chunker.chunk(extraction_result2["text"])
# 	embeddings1 = [[1, 0]] * len(chunks1)  # Dummy embeddings
# 	embeddings2 = [[1, 0]] * len(chunks2)  # Dummy embeddings
# 	db.insert_chunks(chunks1, embeddings1, extraction_result1["metadata"])
# 	db.insert_chunks(chunks2, embeddings2, extraction_result2["metadata"])
# 	assert db.chunks == [{"text": chunk["text"], "embedding": [1, 0], "metadata": extraction_result2["metadata"]} for chunk in chunks2]

# # Edge case: Chunker returns zero chunks
# def test_ingestion_zero_chunks():
# 	class DummyExtractor:
# 		def extract(self, file_path):
# 			return {"text": "Text", "metadata": {"title": "Zero", "page_count": 1, "source_url": ""}}
# 	class ZeroChunker:
# 		def chunk(self, text):
# 			return []
# 	extractor = DummyExtractor()
# 	chunker = ZeroChunker()
# 	db = FakeVectorDatabase()
# 	extraction_result = extractor.extract("/tmp/zero.pdf")
# 	chunks = chunker.chunk(extraction_result["text"])
# 	assert chunks == []

# # Edge case: DB storage failure
# def test_ingestion_db_failure():
# 	class DummyExtractor:
# 		def extract(self, file_path):
# 			return {"text": "Text", "metadata": {"title": "Fail", "page_count": 1, "source_url": ""}}
# 	class DummyChunker:
# 		def chunk(self, text):
# 			return [{"text": text, "chunk_index": 0}]
# 	class FailingDB(FakeVectorDatabase):
# 		def insert_chunks(self, chunks, embeddings, metadata):
# 			return False
# 	extractor = DummyExtractor()
# 	chunker = DummyChunker()
# 	db = FailingDB()
# 	extraction_result = extractor.extract("/tmp/fail.pdf")
# 	chunks = chunker.chunk(extraction_result["text"])
# 	embeddings = [[1, 0]] * len(chunks)
# 	success = db.insert_chunks(chunks, embeddings, extraction_result["metadata"])
# 	assert not success

# # Edge case: Partial extraction
# def test_ingestion_partial_extraction():
# 	class PartialExtractor:
# 		def extract(self, file_path):
# 			return {"text": "Partial", "metadata": {"title": "Partial", "page_count": 1}}
# 	extractor = PartialExtractor()
# 	extraction_result = extractor.extract("/tmp/partial.pdf")
# 	assert extraction_result["text"] == "Partial"

# # Edge case: Batch ingestion (multiple docs)
# def test_ingestion_batch_documents():
# 	class DummyExtractor:
# 		def extract(self, file_path):
# 			return {"text": f"Text-{file_path}", "metadata": {"title": file_path, "page_count": 1}}
# 	class DummyChunker:
# 		def chunk(self, text):
# 			return [{"text": text, "chunk_index": 0}]
# 	db = FakeVectorDatabase()
# 	extractor = DummyExtractor()
# 	chunker = DummyChunker()
# 	docs = ["/tmp/doc1.pdf", "/tmp/doc2.pdf"]
# 	for doc in docs:
# 		extraction_result = extractor.extract(doc)
# 		chunks = chunker.chunk(extraction_result["text"])
# 		embeddings = [[1, 0]] * len(chunks)  # Dummy embeddings
# 		db.insert_chunks(chunks, embeddings, extraction_result["metadata"])
# 	assert all(chunk["chunk_index"] == 0 for chunk in db.chunks)

# # Edge case: Document with images/non-text content
# def test_ingestion_images_in_document():
# 	class ImageExtractor:
# 		def extract(self, file_path):
# 			return {"text": "", "metadata": {"title": "ImageDoc", "page_count": 1}}
# 	extractor = ImageExtractor()
# 	extraction_result = extractor.extract("/tmp/image.pdf")
# 	assert extraction_result["text"] == ""

# # Edge case: Invalid input types
# def test_ingestion_invalid_input():
# 	class DummyExtractor:
# 		def extract(self, file_path):
# 			if not isinstance(file_path, str):
# 				raise TypeError("file_path must be str")
# 			return {"text": "Text", "metadata": {}}
# 	extractor = DummyExtractor()
# 	try:
# 		extractor.extract(123)
# 		assert False, "Should have raised TypeError"
# 	except TypeError:
# 		pass

# # Edge case: Slow extraction or chunking
# def test_ingestion_slow_extraction():
# 	import time
# 	class SlowExtractor:
# 		def extract(self, file_path):
# 			time.sleep(0.1)
# 			return {"text": "Text", "metadata": {}}
# 	extractor = SlowExtractor()
# 	start = time.time()
# 	extraction_result = extractor.extract("/tmp/slow.pdf")
# 	elapsed = time.time() - start
# 	assert elapsed >= 0.1

# # Edge case: Metadata mismatch
# def test_ingestion_metadata_mismatch():
# 	class MismatchExtractor:
# 		def extract(self, file_path):
# 			return {"text": "Text", "metadata": {"title": "Wrong", "page_count": 99}}
# 	extractor = MismatchExtractor()
# 	extraction_result = extractor.extract("/tmp/mismatch.pdf")
# 	assert extraction_result["metadata"]["title"] == "Wrong"

# # --- FastAPI endpoint integration tests ---
# @pytest.mark.skipif(app is None, reason="FastAPI app not importable")
# def test_api_home():
#     client = TestClient(app)
#     resp = client.get("/")
#     assert resp.status_code == 200
#     assert resp.json()["message"].startswith("Ingestion Service")

# @pytest.mark.skipif(app is None, reason="FastAPI app not importable")
# def test_api_health():
#     client = TestClient(app)
#     resp = client.get("/health")
#     assert resp.status_code in (200, 503)
#     assert "status" in resp.json()
#     assert "rabbitmq" in resp.json()["dependencies"]

# @pytest.mark.skipif(app is None or storage is None, reason="App or storage not importable")
# def test_api_documents_empty():
#     client = TestClient(app)
#     # Ensure storage is empty for test
#     if hasattr(storage, 'index'):
#         storage.index.clear()
#         storage._save_index()
#     resp = client.get("/documents")
#     assert resp.status_code == 200
#     assert "documents" in resp.json()
#     assert resp.json()["documents"] == []

# @pytest.mark.skipif(app is None or storage is None, reason="App or storage not importable")
# def test_api_documents_add_and_get():
#     client = TestClient(app)
#     # Add a dummy document to storage
#     doc_id = "testdoc"
#     pdf_bytes = b"%PDF-1.4 test pdf content"
#     metadata = {"url": "http://example.com/test.pdf", "hash": "abc123", "date": "2025-11-27", "correlation_id": "test-corr"}
#     storage.store_document(doc_id, pdf_bytes, metadata)
#     resp = client.get("/documents")
#     assert resp.status_code == 200
#     docs = resp.json()["documents"]
#     assert any(d["document_id"] == doc_id for d in docs)
#     # Test download endpoint
#     resp2 = client.get(f"/documents/{doc_id}")
#     assert resp2.status_code == 200
#     assert resp2.headers["content-type"] == "application/pdf"
#     assert resp2.content.startswith(b"%PDF-1.4")

# @pytest.mark.skipif(app is None, reason="FastAPI app not importable")
# def test_api_discovery_start():
#     client = TestClient(app)
#     resp = client.post("/discovery/start")
#     assert resp.status_code == 200
#     data = resp.json()
#     assert data["message"].startswith("Document discovery started")
#     assert data["job_status"] == "running"

def test_placeholder():
    pass